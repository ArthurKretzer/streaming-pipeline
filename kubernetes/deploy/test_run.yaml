apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: streaming-pipeline-kafka-avro-to-delta
  namespace: spark-jobs
spec:
  timeToLiveSeconds: 3600
  deps:
    repositories:
      - https://repo1.maven.org/maven2
    packages:
      - org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1
      - io.delta:delta-spark_2.12:3.2.0
      - org.apache.spark:spark-avro_2.12:3.5.1
      - org.apache.hadoop:hadoop-aws:3.3.4
      - com.amazonaws:aws-java-sdk-bundle:1.12.262
      - org.apache.hadoop:hadoop-common:3.3.4
      - org.apache.hadoop:hadoop-hdfs-client:3.3.4
      - org.apache.hadoop:hadoop-auth:3.3.4
      - org.apache.hadoop:hadoop-annotations:3.3.4
      - com.github.luben:zstd-jni:1.4.5-4
      - org.xerial.snappy:snappy-java:1.1.7.3
      - org.apache.commons:commons-compress:1.20
      - org.apache.hadoop:hadoop-client-api:3.3.4
      - org.apache.hadoop:hadoop-client-runtime:3.3.4
  sparkConf:
    spark.hadoop.fs.s3a.endpoint: "http://minio.deepstorage.svc.Cluster.local"
    spark.hadoop.fs.s3a.impl: "org.apache.hadoop.fs.s3a.S3AFileSystem"
    spark.sql.extensions: "io.delta.sql.DeltaSparkSessionExtension"
    spark.sql.catalog.spark_catalog: "org.apache.spark.sql.delta.catalog.DeltaCatalog"
    spark.hadoop.fs.s3a.path.style.access: "True"
    spark.hadoop.fs.s3a.fast.upload: "True"
    spark.hadoop.fs.s3a.multipart.size: "104857600"
    spark.hadoop.fs.s3a.connection.maximum: "100"
    spark.driver.extraJavaOptions: "-Divy.cache.dir=/tmp/ivy2/cache -Divy.home=/tmp/ivy2"
    spark.executor.extraJavaOptions: "-Divy.cache.dir=/tmp/ivy2/cache -Divy.home=/tmp/ivy2"
  type: Python
  mode: cluster
  image: "docker.io/bitnami/spark:3.5.4"
  imagePullPolicy: Always
  mainApplicationFile: "local:///opt/spark/examples/src/main/python/pi.py"
  arguments:
    - "10"
  sparkVersion: "3.5.4"
  restartPolicy:
    type: OnFailure
    onFailureRetries: 3
    onFailureRetryInterval: 10
    onSubmissionFailureRetries: 5
    onSubmissionFailureRetryInterval: 20
  driver:
    envSecretKeyRefs:
      MINIO_ACCESS_KEY:
        name: spark-minio-credentials
        key: accessKey
      MINIO_SECRET_KEY:
        name: spark-minio-credentials
        key: secretKey
    cores: 1
    coreLimit: "1200m"
    memory: "512m"
    serviceAccount: spark-operator-spark
    volumeMounts:
      - name: spark-history-volume
        mountPath: /opt/bitnami/spark/logs/events
      - name: spark-history-volume
        mountPath: /opt/bitnami/spark/metrics
      - name: spark-defaults-config
        mountPath: /opt/bitnami/spark/conf
  executor:
    envSecretKeyRefs:
      MINIO_ACCESS_KEY:
        name: spark-minio-credentials
        key: accessKey
      MINIO_SECRET_KEY:
        name: spark-minio-credentials
        key: secretKey
    cores: 1
    instances: 3
    memory: "1g"
    volumeMounts:
      - name: spark-history-volume
        mountPath: /opt/bitnami/spark/logs/events
      - name: spark-history-volume
        mountPath: /opt/bitnami/spark/metrics
      - name: spark-defaults-config
        mountPath: /opt/bitnami/spark/conf
  monitoring:
    exposeDriverMetrics: true
    exposeExecutorMetrics: true
    prometheus:
      jmxExporterJar: /opt/bitnami/spark/jars/jmx_prometheus_javaagent-0.12.0.jar
      port: 8090
  volumes:
    - name: spark-history-volume
      persistentVolumeClaim:
        claimName: pvc-spark-history-server