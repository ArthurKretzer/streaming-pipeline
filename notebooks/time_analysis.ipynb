{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--packages org.apache.hadoop:hadoop-aws:3.3.4,io.delta:delta-spark_2.12:3.3.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.4,org.apache.kafka:kafka-clients:3.9.0,org.apache.spark:spark-avro_2.12:3.5.1 pyspark-shell\"\n",
    "\n",
    "# Initialize Spark session with Delta Lake and MinIO support\n",
    "spark = (SparkSession.builder \\\n",
    "    .appName(\"DeltaLakeWithMinIO\") \\\n",
    "    ## Delta\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    #Hive Catalog\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    ## Optimize Delta\n",
    "    .config(\"delta.autoOptimize.optimizeWrite\", \"true\") \\\n",
    "    .config(\"delta.autoOptimize.autoCompact\", \"true\") \\\n",
    "    .config(\"spark.delta.logStore.class\", \"org.apache.spark.sql.delta.storage.S3SingleDriverLogStore\") \\\n",
    "    ## MinIO\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", os.getenv(\"MINIO_ENDPOINT\")) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", os.getenv(\"MINIO_ACCESS_KEY\")) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", os.getenv(\"MINIO_SECRET_KEY\")) \\\n",
    "    .config('spark.hadoop.fs.s3a.attempts.maximum', \"3\") \\\n",
    "    .config('spark.hadoop.fs.s3a.connection.timeout', \"10000\") \\\n",
    "    .config('spark.hadoop.fs.s3a.connection.establish.timeout', \"5000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3n.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spark_schema():\n",
    "    with open(\"../src/schemas/control_power.json\") as file:\n",
    "        avro_schema_str = file.read()\n",
    "\n",
    "    return avro_schema_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, expr\n",
    "from pyspark.sql.avro.functions import from_avro\n",
    "\n",
    "# Kafka Configuration\n",
    "kafka_broker = \"kafka-cpc.certi.org.br:31289\"\n",
    "topic_name = \"control_power-avro\"\n",
    "\n",
    "# Read data from Kafka\n",
    "kafka_stream = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n",
    "    .option(\"subscribe\", topic_name) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Deserialize Kafka value (JSON string) into columns\n",
    "parsed_stream = kafka_stream.select(\n",
    "        \"timestamp\",\n",
    "        from_avro(\n",
    "            expr(\"substring(value, 6, length(value)-5)\"),\n",
    "            get_spark_schema()\n",
    "        ).alias(\"data\")\n",
    "    ) \\\n",
    "    .select(\"timestamp\", \"data.*\")\n",
    "\n",
    "parsed_stream_with_timestamp = parsed_stream.withColumn(\"landing_timestamp\", current_timestamp())\n",
    "\n",
    "# Output the parsed stream for verification\n",
    "parsed_stream_with_timestamp.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parsed_stream_with_timestamp.toPandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df[\"source_timestamp\"] = pd.to_datetime(df[\"source_timestamp\"]).dt.tz_convert(\"America/Sao_Paulo\")\n",
    "df[\"landing_timestamp\"] = df[\"landing_timestamp\"].dt.tz_localize(\"America/Sao_Paulo\")\n",
    "df[\"timestamp\"] = df[\"timestamp\"].dt.tz_localize(\"America/Sao_Paulo\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"source_kafka_latency\"] = df[\"timestamp\"] - df[\"source_timestamp\"]\n",
    "df[\"source_kafka_latency\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Convert timedelta to seconds for better visualization\n",
    "df[\"source_kafka_latency_sec\"] = df[\"source_kafka_latency\"].dt.total_seconds()\n",
    "\n",
    "# Plot histogram\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(df[\"source_kafka_latency_sec\"], bins=50, edgecolor=\"black\")\n",
    "plt.xlabel(\"Latency (seconds)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Kafka Source Latency\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.boxplot(df[\"source_kafka_latency_sec\"], vert=False)\n",
    "plt.xlabel(\"Latency (seconds)\")\n",
    "plt.title(\"Box Plot of Kafka Source Latency\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT timestamp, landing_timestamp, parsed_value.* FROM delta.`s3a://lakehouse/delta/raw_control_power-avro`;\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datalake = spark.sql(\"SELECT timestamp, landing_timestamp, parsed_value.* FROM delta.`s3a://lakehouse/delta/raw_control_power-avro`;\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_datalake[\"source_timestamp\"] = pd.to_datetime(df_datalake[\"source_timestamp\"]).dt.tz_convert(\"America/Sao_Paulo\")\n",
    "df_datalake[\"landing_timestamp\"] = df_datalake[\"landing_timestamp\"].dt.tz_localize(\"America/Sao_Paulo\")\n",
    "df_datalake[\"timestamp\"] = df_datalake[\"timestamp\"].dt.tz_localize(\"America/Sao_Paulo\")\n",
    "\n",
    "df_datalake[\"source_kafka_latency\"] = df_datalake[\"timestamp\"] - df_datalake[\"source_timestamp\"]\n",
    "df_datalake[\"source_kafka_latency\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_datalake[\"kafka_landing_latency\"] = df_datalake[\"landing_timestamp\"] - df_datalake[\"timestamp\"]\n",
    "df_datalake[\"kafka_landing_latency\"].describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
